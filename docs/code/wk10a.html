<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #3D7B7B; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
body .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
body .cp { color: #9C6500 } /* Comment.Preproc */
body .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
body .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #E40000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #008400 } /* Generic.Inserted */
body .go { color: #717171 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #687822 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #717171; font-weight: bold } /* Name.Entity */
body .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #767600 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #A45A77 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<div class="highlight"><pre><span></span><span class="c1">## Train a bigram tagger with no backoff tagger, and run it on some of</span>
<span class="c1">## the training data. Next, run it on some new data. What happens to</span>
<span class="c1">## the performance of the tagger? Why?</span>
      

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">tagged</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">treebank</span><span class="o">.</span><span class="n">tagged_sents</span><span class="p">()</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">tagged</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>

<span class="n">ln</span> <span class="o">=</span>  <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">tagged</span><span class="p">))</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">tagged</span><span class="p">[:</span><span class="n">ln</span><span class="p">]</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">tagged</span><span class="p">[</span><span class="n">ln</span><span class="p">:]</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Training Bigram Tagger&quot;</span><span class="p">)</span>
<span class="n">bigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">BigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">bigram_tagger</span><span class="o">.</span><span class="n">tag</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="s1">&#39;The police reported it.&#39;</span><span class="p">)))</span>

<span class="nb">print</span> <span class="p">(</span><span class="n">bigram_tagger</span><span class="o">.</span><span class="n">tag</span><span class="p">(</span><span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="s1">&#39;The police saw the report.&#39;</span><span class="p">)))</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test: &quot;</span><span class="p">,</span> <span class="n">bigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
<span class="c1">#0.13475826972010177</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Train:&quot;</span><span class="p">,</span> <span class="n">bigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">train</span><span class="p">))</span>
<span class="c1">#0.91111820453269643</span>

<span class="c1">### Now add a backoff tagger (your choice) and try it again (build</span>
<span class="c1">### then evaluate). What happens to the performance of the tagger this</span>
<span class="c1">### time and why?</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Unigram Tagger&quot;</span><span class="p">)</span>
<span class="n">backoff_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">UnigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test: &quot;</span><span class="p">,</span> <span class="n">backoff_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
<span class="c1">#0.86493638676844786</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Train:&quot;</span><span class="p">,</span> <span class="n">backoff_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">train</span><span class="p">))</span>
<span class="c1">#0.96015453875026147</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Bigram Tagger with backoff to unigram&quot;</span><span class="p">)</span>
<span class="n">bigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">BigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">backoff_tagger</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test: &quot;</span><span class="p">,</span> <span class="n">bigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
<span class="c1">#0.87430025445292625</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Bigram Tagger with backoff to NNP (Proper Noun)&quot;</span><span class="p">)</span>
<span class="n">backoff_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">DefaultTagger</span><span class="p">(</span><span class="s1">&#39;NNP&#39;</span><span class="p">)</span>
<span class="n">unigram_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">UnigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">backoff_tagger</span><span class="p">)</span>
<span class="n">bigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">BigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">unigram_tagger</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test (bigram/NNP): &quot;</span><span class="p">,</span> <span class="n">bigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
<span class="c1">#0.89954198473282443</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Bigram Tagger with backoff to NN (Common Noun)&quot;</span><span class="p">)</span>
<span class="n">backoff_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">DefaultTagger</span><span class="p">(</span><span class="s1">&#39;NN&#39;</span><span class="p">)</span>
<span class="n">unigram_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">UnigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">backoff_tagger</span><span class="p">)</span>
<span class="n">bigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">BigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">unigram_tagger</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test (bigram/NN):&quot;</span><span class="p">,</span> <span class="n">bigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
<span class="c1"># 0.89251908396946567</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Bigram Tagger with backoff to NNP (Proper Noun)&quot;</span><span class="p">)</span>
<span class="n">backoff_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">DefaultTagger</span><span class="p">(</span><span class="s1">&#39;NNP&#39;</span><span class="p">)</span>
<span class="n">unigram_tagger</span><span class="o">=</span><span class="n">nltk</span><span class="o">.</span><span class="n">UnigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">backoff_tagger</span><span class="p">)</span>
<span class="n">bigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">BigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">unigram_tagger</span><span class="p">)</span>
<span class="n">trigram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">TrigramTagger</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">bigram_tagger</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test (trigram/NNP): &quot;</span><span class="p">,</span> <span class="n">trigram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>

<span class="n">fourgram_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">NgramTagger</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">backoff</span><span class="o">=</span><span class="n">trigram_tagger</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test (4gram/NNP): &quot;</span><span class="p">,</span> <span class="n">fourgram_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>

<span class="n">perceptron_tagger</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">PerceptronTagger</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy on Test (Perceptron): &quot;</span><span class="p">,</span> <span class="n">perceptron_tagger</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>






<span class="c1">#0.89954198473282443</span>



<span class="c1">## NNP is the better backoff</span>
</pre></div>
</body>
</html>
